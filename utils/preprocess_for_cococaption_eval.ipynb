{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\":\n",
    "    \"could've\", \"couldnt\": \"couldn't\", \"couldn'tve\": \"couldn't've\",\n",
    "    \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\":\n",
    "    \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \"hadnt've\":\n",
    "    \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\":\n",
    "    \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \"he'dve\":\n",
    "    \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\",\n",
    "    \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \"Im\":\n",
    "    \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\":\n",
    "    \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\",\n",
    "    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\":\n",
    "    \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\",\n",
    "    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\",\n",
    "    \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\",\n",
    "    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\":\n",
    "    \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\":\n",
    "    \"she'd've\", \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\":\n",
    "    \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\":\n",
    "    \"shouldn't've\", \"somebody'd\": \"somebodyd\", \"somebodyd've\":\n",
    "    \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\":\n",
    "    \"somebody'll\", \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\",\n",
    "    \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\",\n",
    "    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\":\n",
    "    \"something'd\", \"somethingd've\": \"something'd've\", \"something'dve\":\n",
    "    \"something'd've\", \"somethingll\": \"something'll\", \"thats\":\n",
    "    \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\",\n",
    "    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\":\n",
    "    \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \"they'dve\":\n",
    "    \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\":\n",
    "    \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \"wed've\":\n",
    "    \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\":\n",
    "    \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \"whats\":\n",
    "    \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\":\n",
    "    \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \"whod\":\n",
    "    \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\":\n",
    "    \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\",\n",
    "    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\":\n",
    "    \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\",\n",
    "    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\":\n",
    "    \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\",\n",
    "    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\":\n",
    "    \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \"youll\":\n",
    "    \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"\n",
    "}\n",
    "\n",
    "manual_map = {'none': '0',\n",
    "              'zero': '0',\n",
    "              'one': '1',\n",
    "              'two': '2',\n",
    "              'three': '3',\n",
    "              'four': '4',\n",
    "              'five': '5',\n",
    "              'six': '6',\n",
    "              'seven': '7',\n",
    "              'eight': '8',\n",
    "               'nine': '9',\n",
    "              'ten': '10'}\n",
    "articles = ['a', 'an', 'the']\n",
    "period_strip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "comma_strip = re.compile(\"(\\d)(\\,)(\\d)\")\n",
    "punct = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "                '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "                '>', '<', '@', '`', ',', '?', '!']\n",
    "\n",
    "def process_punctuation(inText):\n",
    "    outText = inText\n",
    "    for p in punct:\n",
    "        if (p + ' ' in inText or ' ' + p in inText) \\\n",
    "           or (re.search(comma_strip, inText) != None):\n",
    "            outText = outText.replace(p, '')\n",
    "        else:\n",
    "            outText = outText.replace(p, ' ')\n",
    "    outText = period_strip.sub(\"\", outText, re.UNICODE)\n",
    "    return outText\n",
    "\n",
    "\n",
    "def process_digit_article(inText):\n",
    "    outText = []\n",
    "    tempText = inText.lower().split()\n",
    "    for word in tempText:\n",
    "        word = manual_map.setdefault(word, word)\n",
    "        if word not in articles:\n",
    "            outText.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    for wordId, word in enumerate(outText):\n",
    "        if word in contractions:\n",
    "            outText[wordId] = contractions[word]\n",
    "    outText = ' '.join(outText)\n",
    "    return outText\n",
    "\n",
    "\n",
    "def prep_ans(answer):\n",
    "    answer = process_digit_article(process_punctuation(answer))\n",
    "    answer = answer.replace(',', '')\n",
    "    return answer\n",
    "\n",
    "\n",
    "def proc_ans(ans):\n",
    "\n",
    "    ans_prob_dict = {}\n",
    "\n",
    "    for ans_ in ans:\n",
    "        ans_proc = prep_ans(ans_['answer'])\n",
    "        if ans_proc not in ans_prob_dict:\n",
    "            ans_prob_dict[ans_proc] = 1\n",
    "        else:\n",
    "            ans_prob_dict[ans_proc] += 1\n",
    "\n",
    "    confident_answer = max(ans_prob_dict, key=ans_prob_dict.get)\n",
    "    return confident_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQA-X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From ruotian luo\n",
    "Create a reference json file used for evaluation with `coco-caption` repo.\n",
    "Used when multiple evaluation captions exist for a single image\n",
    "\"\"\"\n",
    "# Explanations only\n",
    "\n",
    "split = 'test'\n",
    "in_path = 'nle_data/VQA-X/vqaX_' + split + '.json'\n",
    "out_path = 'nle_data/VQA-X/vqaX_' + split + '_annot_exp.json'\n",
    "data = json.load(open(in_path, 'r'))\n",
    "\n",
    "out = {'info': {'description': 'This is stable 1.0 version of the 2014 MS COCO dataset.', 'url': 'http://mscoco.org', 'version': '1.0', 'year': 2014, 'contributor': 'Microsoft COCO group', 'date_created': '2015-01-27 09:11:52.357475'}, 'licenses': [{'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/', 'id': 1, 'name': 'Attribution-NonCommercial-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nc/2.0/', 'id': 2, 'name': 'Attribution-NonCommercial License'}, {'url': 'http://creativecommons.org/licenses/by-nc-nd/2.0/', 'id': 3, 'name': 'Attribution-NonCommercial-NoDerivs License'}, {'url': 'http://creativecommons.org/licenses/by/2.0/', 'id': 4, 'name': 'Attribution License'}, {'url': 'http://creativecommons.org/licenses/by-sa/2.0/', 'id': 5, 'name': 'Attribution-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nd/2.0/', 'id': 6, 'name': 'Attribution-NoDerivs License'}, {'url': 'http://flickr.com/commons/usage/', 'id': 7, 'name': 'No known copyright restrictions'}, {'url': 'http://www.usa.gov/copyright.shtml', 'id': 8, 'name': 'United States Government Work'}], 'type': 'captions'}\n",
    "out.update({'images': [], 'annotations': []})\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for qid, qid_data in data.items():\n",
    "    \n",
    "    out['images'].append({'id': int(qid)})\n",
    "    \n",
    "    for s in qid_data['explanation']:\n",
    "        \n",
    "        if len(s) == 0:\n",
    "            print(\"Warning: {} has no annotations\".format(qid))\n",
    "            continue\n",
    "            \n",
    "        out['annotations'].append({'image_id': out['images'][-1]['id'], 'caption': s, 'id': cnt})\n",
    "        cnt += 1\n",
    "\n",
    "json.dump(out, open(out_path, 'w'))\n",
    "print('wrote to ', out_path)\n",
    "\n",
    "# Explanations + Answers\n",
    "\n",
    "split = 'test'\n",
    "in_path = 'nle_data/VQA-X/vqaX_' + split + '.json'\n",
    "out_path = 'nle_data/VQA-X/vqaX_' + split + '_annot_full.json'\n",
    "\n",
    "data = json.load(open(in_path, 'r'))\n",
    "\n",
    "out = {'info': {'description': 'This is stable 1.0 version of the 2014 MS COCO dataset.', 'url': 'http://mscoco.org', 'version': '1.0', 'year': 2014, 'contributor': 'Microsoft COCO group', 'date_created': '2015-01-27 09:11:52.357475'}, 'licenses': [{'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/', 'id': 1, 'name': 'Attribution-NonCommercial-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nc/2.0/', 'id': 2, 'name': 'Attribution-NonCommercial License'}, {'url': 'http://creativecommons.org/licenses/by-nc-nd/2.0/', 'id': 3, 'name': 'Attribution-NonCommercial-NoDerivs License'}, {'url': 'http://creativecommons.org/licenses/by/2.0/', 'id': 4, 'name': 'Attribution License'}, {'url': 'http://creativecommons.org/licenses/by-sa/2.0/', 'id': 5, 'name': 'Attribution-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nd/2.0/', 'id': 6, 'name': 'Attribution-NoDerivs License'}, {'url': 'http://flickr.com/commons/usage/', 'id': 7, 'name': 'No known copyright restrictions'}, {'url': 'http://www.usa.gov/copyright.shtml', 'id': 8, 'name': 'United States Government Work'}], 'type': 'captions'}\n",
    "out.update({'images': [], 'annotations': []})\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for qid, qid_data in data.items():\n",
    "    \n",
    "    out['images'].append({'id': int(qid)})\n",
    "    for s in qid_data['explanation']:\n",
    "        \n",
    "        if len(s) == 0:\n",
    "            print(\"Warning: {} has no annotations\".format(qid))\n",
    "            continue\n",
    "            \n",
    "        s = proc_ans(qid_data['answers']) + \" because \" + s\n",
    "        out['annotations'].append({'image_id': out['images'][-1]['id'], 'caption': s, 'id': cnt})\n",
    "        cnt += 1\n",
    "\n",
    "json.dump(out, open(out_path, 'w'))\n",
    "print('wrote to ', out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACT-X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanations\n",
    "\n",
    "in_path = 'nle_data/ACT-X/actX_test.json'\n",
    "out_path = 'nle_data/ACT-X/actX_test_annot_exp.json'\n",
    "data = json.load(open(in_path, 'r'))\n",
    "\n",
    "out = {'info': {'description': 'This is stable 1.0 version of the 2014 MS COCO dataset.', 'url': 'http://mscoco.org', 'version': '1.0', 'year': 2014, 'contributor': 'Microsoft COCO group', 'date_created': '2015-01-27 09:11:52.357475'}, 'licenses': [{'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/', 'id': 1, 'name': 'Attribution-NonCommercial-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nc/2.0/', 'id': 2, 'name': 'Attribution-NonCommercial License'}, {'url': 'http://creativecommons.org/licenses/by-nc-nd/2.0/', 'id': 3, 'name': 'Attribution-NonCommercial-NoDerivs License'}, {'url': 'http://creativecommons.org/licenses/by/2.0/', 'id': 4, 'name': 'Attribution License'}, {'url': 'http://creativecommons.org/licenses/by-sa/2.0/', 'id': 5, 'name': 'Attribution-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nd/2.0/', 'id': 6, 'name': 'Attribution-NoDerivs License'}, {'url': 'http://flickr.com/commons/usage/', 'id': 7, 'name': 'No known copyright restrictions'}, {'url': 'http://www.usa.gov/copyright.shtml', 'id': 8, 'name': 'United States Government Work'}], 'type': 'captions'}\n",
    "out.update({'images': [], 'annotations': []})\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for imgid, img_data in data.items():\n",
    "    \n",
    "    out['images'].append({'id': int(imgid)})\n",
    "    \n",
    "    for s in img_data['explanation']:\n",
    "        \n",
    "        if len(s) == 0:\n",
    "            print(\"Warning: {} has no annotations\".format(imgid))\n",
    "            continue\n",
    "            \n",
    "        out['annotations'].append({'image_id': out['images'][-1]['id'], 'caption': s, 'id': cnt})\n",
    "        cnt += 1\n",
    "\n",
    "json.dump(out, open(out_path, 'w'))\n",
    "print('wrote to ', out_path)\n",
    "\n",
    "# Explanations + Answers\n",
    "\n",
    "in_path = 'nle_data/ACT-X/actX_test.json'\n",
    "out_path = 'nle_data/ACT-X/actX_test_annot_full.json'\n",
    "data = json.load(open(in_path, 'r'))\n",
    "\n",
    "out = {'info': {'description': 'This is stable 1.0 version of the 2014 MS COCO dataset.', 'url': 'http://mscoco.org', 'version': '1.0', 'year': 2014, 'contributor': 'Microsoft COCO group', 'date_created': '2015-01-27 09:11:52.357475'}, 'licenses': [{'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/', 'id': 1, 'name': 'Attribution-NonCommercial-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nc/2.0/', 'id': 2, 'name': 'Attribution-NonCommercial License'}, {'url': 'http://creativecommons.org/licenses/by-nc-nd/2.0/', 'id': 3, 'name': 'Attribution-NonCommercial-NoDerivs License'}, {'url': 'http://creativecommons.org/licenses/by/2.0/', 'id': 4, 'name': 'Attribution License'}, {'url': 'http://creativecommons.org/licenses/by-sa/2.0/', 'id': 5, 'name': 'Attribution-ShareAlike License'}, {'url': 'http://creativecommons.org/licenses/by-nd/2.0/', 'id': 6, 'name': 'Attribution-NoDerivs License'}, {'url': 'http://flickr.com/commons/usage/', 'id': 7, 'name': 'No known copyright restrictions'}, {'url': 'http://www.usa.gov/copyright.shtml', 'id': 8, 'name': 'United States Government Work'}], 'type': 'captions'}\n",
    "out.update({'images': [], 'annotations': []})\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for imgid, img_data in data.items():\n",
    "    \n",
    "    out['images'].append({'id': int(imgid)})\n",
    "    \n",
    "    for s in img_data['explanation']:\n",
    "        \n",
    "        if len(s) == 0:\n",
    "            print(\"Warning: {} has no annotations\".format(imgid))\n",
    "            continue\n",
    "            \n",
    "        s = prep_ans(img_data['answers']) + \" because \" + s\n",
    "        out['annotations'].append({'image_id': out['images'][-1]['id'], 'caption': s, 'id': cnt})\n",
    "        cnt += 1\n",
    "\n",
    "json.dump(out, open(out_path, 'w'))\n",
    "print('wrote to ', out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eSNLI-VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanations \n",
    "\n",
    "in_path = 'nle_data/eSNLI-VE/esnlive_test.json'\n",
    "out_path = 'nle_data/eSNLI-VE/esnlive_test_annot_exp.json'\n",
    "data = json.load(open(in_path, 'r'))\n",
    "\n",
    "test = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'info': {'description': 'eSNLI-VE (test split)'},\n",
    "    'type': 'captions',\n",
    "    'licenses': 'http://creativecommons.org/licenses/by/4.0/',\n",
    "}\n",
    "\n",
    "unique_ids = []\n",
    "\n",
    "for imgid, (key,value) in enumerate(data.items()):\n",
    "    \n",
    "    # Extract info\n",
    "    url           = 'https://github.com/maximek3/e-ViL'             \n",
    "    filename      = value['image_name']\n",
    "    id            = int(key)                    \n",
    "    explanation   = value['explanation']\n",
    "\n",
    "    # Skip duplicate paragraph captions\n",
    "    if id in unique_ids: \n",
    "        print(\"Found duplicate!\")\n",
    "        break   # this format assumes only 1 caption per image, otherwise cider calculation goes wrong\n",
    "    else:\n",
    "        unique_ids.append(id)\n",
    "\n",
    "    # Extract image info\n",
    "    image = {\n",
    "        'url': url,\n",
    "        'file_name': filename,\n",
    "        'id': id,\n",
    "    }\n",
    "\n",
    "    # Extract caption info\n",
    "    annotation = {\n",
    "        'image_id': id,\n",
    "        'id': imgid,\n",
    "        'caption': explanation\n",
    "    }\n",
    "\n",
    "    # Store info\n",
    "    test['images'].append(image)\n",
    "    test['annotations'].append(annotation)\n",
    "    \n",
    "json.dump(test, open(out_path, 'w'))\n",
    "print('wrote to ', out_path)\n",
    "\n",
    "# Explanations + Answers\n",
    "\n",
    "in_path = 'nle_data/eSNLI-VE/esnlive_test.json'\n",
    "out_path = 'nle_data/eSNLI-VE/esnlive_test_annot_full.json'\n",
    "data = json.load(open(in_path, 'r'))\n",
    "\n",
    "test = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'info': {'description': 'eSNLI-VE (test split)'},\n",
    "    'type': 'captions',\n",
    "    'licenses': 'http://creativecommons.org/licenses/by/4.0/',\n",
    "}\n",
    "\n",
    "unique_ids = []\n",
    "\n",
    "for imgid, (key,value) in enumerate(data.items()):\n",
    "    \n",
    "    # Extract info\n",
    "    url           = 'https://github.com/maximek3/e-ViL'             \n",
    "    filename      = value['image_name']\n",
    "    id            = int(key)                    \n",
    "    explanation   = value['answers'] + \" because \" + value['explanation']\n",
    "\n",
    "    # Skip duplicate paragraph captions\n",
    "    if id in unique_ids: \n",
    "        print(\"Found duplicate!\")\n",
    "        break   # this format assumes only 1 caption per image, otherwise cider calculation goes wrong\n",
    "    else:\n",
    "        unique_ids.append(id)\n",
    "\n",
    "    # Extract image info\n",
    "    image = {\n",
    "        'url': url,\n",
    "        'file_name': filename,\n",
    "        'id': id,\n",
    "    }\n",
    "\n",
    "    # Extract caption info\n",
    "    annotation = {\n",
    "        'image_id': id,\n",
    "        'id': imgid,\n",
    "        'caption': explanation\n",
    "    }\n",
    "\n",
    "    # Store info\n",
    "    test['images'].append(image)\n",
    "    test['annotations'].append(annotation)\n",
    "    \n",
    "json.dump(test, open(out_path, 'w'))\n",
    "print('wrote to ', out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanations \n",
    "\n",
    "in_path = 'vcr_test.json'\n",
    "out_path = 'vcr_test_annot_exp.json'\n",
    "data = json.load(open(in_path, 'r'))\n",
    "\n",
    "test = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'info': {'description': 'VCR (test split)'},\n",
    "    'type': 'captions',\n",
    "    'licenses': 'http://creativecommons.org/licenses/by/4.0/',\n",
    "}\n",
    "\n",
    "unique_ids = []\n",
    "\n",
    "for imgid, (key,value) in enumerate(data.items()):\n",
    "    \n",
    "    # Extract info\n",
    "    url           = 'https://github.com/maximek3/e-ViL'             \n",
    "    filename      = value['img_name']\n",
    "    id            = int(key)                    \n",
    "    explanation   = value['explanation']\n",
    "\n",
    "    # Skip duplicate paragraph captions\n",
    "    if id in unique_ids: \n",
    "        print(\"Found duplicate!\")\n",
    "        break   # this format assumes only 1 caption per image, otherwise cider calculation goes wrong\n",
    "    else:\n",
    "        unique_ids.append(id)\n",
    "\n",
    "    # Extract image info\n",
    "    image = {\n",
    "        'url': url,\n",
    "        'file_name': filename,\n",
    "        'id': id,\n",
    "    }\n",
    "\n",
    "    # Extract caption info\n",
    "    annotation = {\n",
    "        'image_id': id,\n",
    "        'id': imgid,\n",
    "        'caption': explanation\n",
    "    }\n",
    "\n",
    "    # Store info\n",
    "    test['images'].append(image)\n",
    "    test['annotations'].append(annotation)\n",
    "    \n",
    "json.dump(test, open(out_path, 'w'))\n",
    "print('wrote to ', out_path)\n",
    "\n",
    "# Explanations + Answers\n",
    "\n",
    "in_path = 'vcr_test.json'\n",
    "out_path = 'vcr_test_annot_full.json'\n",
    "data = json.load(open(in_path, 'r'))\n",
    "\n",
    "test = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'info': {'description': 'eSNLI-VE (test split)'},\n",
    "    'type': 'captions',\n",
    "    'licenses': 'http://creativecommons.org/licenses/by/4.0/',\n",
    "}\n",
    "\n",
    "unique_ids = []\n",
    "\n",
    "for imgid, (key,value) in enumerate(data.items()):\n",
    "    \n",
    "    # Extract info\n",
    "    url           = 'https://github.com/maximek3/e-ViL'             \n",
    "    filename      =  value['img_name']\n",
    "    id            =  int(key)                    \n",
    "    explanation   =  value['answers'] + \" because \" + value['explanation']\n",
    "\n",
    "    # Skip duplicate paragraph captions\n",
    "    if id in unique_ids: \n",
    "        print(\"Found duplicate!\")\n",
    "        break   # this format assumes only 1 caption per image, otherwise cider calculation goes wrong\n",
    "    else:\n",
    "        unique_ids.append(id)\n",
    "\n",
    "    # Extract image info\n",
    "    image = {\n",
    "        'url': url,\n",
    "        'file_name': filename,\n",
    "        'id': id,\n",
    "    }\n",
    "\n",
    "    # Extract caption info\n",
    "    annotation = {\n",
    "        'image_id': id,\n",
    "        'id': imgid,\n",
    "        'caption': explanation\n",
    "    }\n",
    "\n",
    "    # Store info\n",
    "    test['images'].append(image)\n",
    "    test['annotations'].append(annotation)\n",
    "    \n",
    "json.dump(test, open(out_path, 'w'))\n",
    "print('wrote to ', out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
